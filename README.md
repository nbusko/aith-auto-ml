# AITH AutoML Course

## Kaggle Competition: Playground Series S4E8 - Mushroom Classification
Было выбрано соревнование **https://www.kaggle.com/competitions/playground-series-s4e8**

## Цель проекта

Проект направлен на решение задачи классификации грибов (съедобные/ядовитые) с использованием машинного обучения. Основные цели:

- **Превзойти результаты baseline на LAMA** (Light Auto ML)
- Продемонстрировать качественный код с использованием стандартных подходов
- Использовать стандартные инструменты организации кода (sklearn Pipeline)
- Провести качественный EDA (Exploratory Data Analysis)
- Предоставить подробное описание и обоснование гипотез

## Результаты

**Финальный Kaggle Score:** **0.98456**

**Kaggle Score для LAMA Baseline:** 0.98224

![Скрин с Kaggle](kaggle/submission.png)

## Структура репозитория

```
aith-auto-ml/
├── mushrooms.ipynb          # Основной ноутбук с полным анализом и моделированием
├── pipeline_utils.py         # Утилиты для предобработки данных в pipeline
├── prompt.md                 # Техническое задание проекта
├── README.md                 # Отчет
├── logs/                     # Логи выполнения
│   └── mushrooms.log
├── results/                  # Результаты анализа и моделирования
│   ├── eda/                  # Результаты EDA
│   │   ├── 01_data_overview.txt
│   │   ├── 02_target_statistics.txt
│   │   ├── 02_target_distribution.csv
│   │   ├── 03_feature_types.json
│   │   ├── 04_outliers_analysis.txt
│   │   ├── 06_missing_values_analysis.txt
│   │   ├── 07_correlations_report.txt
│   │   ├── 08_feature_importance_report.txt
│   │   ├── results/eda/visualizations        # Визуализации
│   │   └── 09_feature_engineering_ideas.txt
│   ├── baseline/             # Результаты LAMA baseline
│   │   ├── models/
│   │   ├── predictions/
│   │   └── submissions/
│   ├── models/               # Обученные модели и pipeline, метрики
│   └── submissions/         # Финальные submission файлы
└── playground-series-s4e8/   # Данные конкурса
    ├── train.csv
    ├── test.csv
    └── sample_submission.csv
```

## Структура ноутбука

Ноутбук `mushrooms.ipynb` организован в 5 основных этапов:

### ЭТАП 1: Подготовка окружения и загрузка данных

**Выполненные задачи:**
- Импорт всех необходимых библиотек (pandas, numpy, logging, pathlib)
- Настройка логирования для отслеживания процесса
- Загрузка данных: train.csv, test.csv, sample_submission.csv
- Проведение первичного осмотра данных и сохранение метаинформации

**Ключевые результаты:**
- **Train set:** 3,116,945 строк, 22 столбца (21 признак + 1 целевая переменная)
- **Test set:** 2,077,964 строк, 21 столбец
- Все данные успешно загружены без ошибок
- Метаинформация сохранена в `results/eda/01_data_overview.txt`

**Выводы:**
1. Датасет большой (более 3 миллионов строк в train), что потребовало оптимизации при обучении моделей
2. Все необходимые файлы присутствуют и корректно загружены
3. Структура данных соответствует ожидаемой

---

### ЭТАП 2: EDA - Анализ целевой переменной

#### 2.1 Численный анализ

**Результаты:**
- Общее количество наблюдений: 3,116,945
- Количество пропущенных значений: 0
- Распределение классов:
  - **p (ядовитые):** 1,705,396 (54.71%)
  - **e (съедобные):** 1,411,549 (45.29%)
- Разница в балансе классов: 9.42%
- **Классы сбалансированы** (разница < 10%)

**Выводы:**
1. Целевая переменная бинарная (e/p)
2. Классы хорошо сбалансированы, что позволяет использовать стандартные метрики
3. Пропущенных значений в целевой переменной нет

#### 2.2 Визуализация

- Созданы pie chart и bar chart распределения классов
- Визуализации сохранены в `results/eda/`

---

### ЭТАП 3: EDA - Анализ признаков

#### 3.1 Типизация признаков

**Результаты:**
- **Численных признаков:** 3
  - `cap-diameter` (диаметр шляпки)
  - `stem-height` (высота ножки)
  - `stem-width` (ширина ножки)
- **Категориальных признаков:** 17
  - `cap-shape`, `cap-surface`, `cap-color`
  - `does-bruise-or-bleed`, `gill-attachment`, `gill-spacing`, `gill-color`
  - `stem-root`, `stem-surface`, `stem-color`
  - `veil-type`, `veil-color`, `has-ring`, `ring-type`
  - `spore-print-color`, `habitat`, `season`

#### 3.2 Визуализация распределений

- Созданы гистограммы для численных признаков
- Созданы bar charts для категориальных признаков
- Все визуализации сохранены в `results/eda/`

#### 3.3 Выявление аномальных значений

**Методы:**
- IQR метод (Interquartile Range)
- Z-score метод (порог = 3)

**Результаты:**
- Выявлены выбросы в численных признаках
- Созданы box plots для визуализации выбросов
- Для категориальных признаков выявлены редкие значения

#### 3.4 Анализ пропущенных значений

**Результаты:**
- Пропущенных значений в данных **не обнаружено**
- Все признаки имеют полные данные

#### 3.5 Анализ зависимостей между признаками

**Методы:**
- Для численных признаков: корреляция Пирсона
- Для категориальных признаков: Cramér's V
- Смешанный анализ: ANOVA F-test

**Ключевые находки:**
- Сильная корреляция между `cap-diameter` и `stem-width`: **0.750**
- Все ANOVA тесты показали значимые связи (p < 0.05)
- Создана корреляционная матрица для численных признаков

#### 3.6 Определение важности признаков

**Методы:**
- Для численных: корреляция с таргетом + Mutual Information
- Для категориальных: Chi-square test + Mutual Information

**Топ-10 важных признаков:**
1. **stem-width** (numeric): combined score = 1.000
2. **stem-root** (categorical): combined score = 1.000
3. **cap-diameter** (numeric): combined score = 0.875
4. **veil-color** (categorical): combined score = 0.848
5. **spore-print-color** (categorical): combined score = 0.593
6. **stem-surface** (categorical): combined score = 0.543
7. **stem-height** (numeric): combined score = 0.389
8. **cap-surface** (categorical): combined score = 0.365
9. **cap-color** (categorical): combined score = 0.345
10. **stem-color** (categorical): combined score = 0.336

#### 3.7 Анализ возможных преобразований и генерации новых признаков

**Проанализированные преобразования:**
- Логарифмирование для признаков с высокой асимметрией
- Стандартизация и нормализация
- Q-Q plots для проверки нормальности

**Идеи для feature engineering:**
- Взаимодействия между важными признаками
- Группировка редких категорий
- Создание комбинированных признаков

---

### ЭТАП 4: Моделирование - LAMA Baseline

#### 4.1 Подготовка данных для LAMA

**Стратегия разделения данных:**
- **Stratified 80/20 split** для сохранения распределения классов
- Проверка на data leakage
- Подготовка данных в формате, требуемом LAMA

#### 4.2 Обучение LAMA - Конфигурация 1

**Конфигурация:**
- Модели: LinearL2, LightGBM, CatBoost
- Timeout: 300 секунд (для тестирования)
- CPU limit: 4

**Результаты:**
- **ROC-AUC:** 0.996311
- **Accuracy:** 0.988700
- **F1-Score:** 0.988702

#### 4.3 Обучение LAMA - Конфигурация 2

**Альтернативная конфигурация:**
- Измененные параметры моделей
- Timeout: 300 секунд

**Результаты:**
- **ROC-AUC:** 0.996286
- **Accuracy:** 0.988563
- **F1-Score:** 0.988565

#### 4.4 Сравнение конфигураций и выбор лучшей

**Выводы:**
- **Лучшая конфигурация:** Config 1 (ROC-AUC: 0.996311)
- Разница между конфигурациями минимальна (0.000025)
- Финальная LAMA модель обучена на полном train set
- **Kaggle Score для LAMA:** 0.98224

**Ключевые инсайты:**
1. LAMA показал отличные результаты - ROC-AUC 0.997 на валидации
2. Автоматический подбор моделей эффективен - LAMA автоматически выбрал лучшие модели и создал blending
3. Kaggle Score ниже валидационного (0.98224 vs 0.997), что указывает на различия в распределении данных

---

### ЭТАП 5: Моделирование - Собственное решение

#### 5.1 Обоснование стратегии разделения данных

- **Stratified 80/20 split** для сохранения баланса классов
- Проверка на data leakage (временные признаки отсутствуют)
- Использование той же стратегии, что и для LAMA baseline

#### 5.2 Pipeline V1: RandomForest + SimpleImputer + RobustScaler + One-Hot Encoding

**Архитектура:**
- Preprocessing:
  - Численные признаки: SimpleImputer (median) + RobustScaler
  - Категориальные признаки: One-Hot Encoding
- Feature Selection: SelectKBest
- Model: RandomForestClassifier

**Результаты (до оптимизации):**
- ROC-AUC: 0.981060
- Accuracy: 0.930942
- F1-Score: 0.930945

#### 5.3 Оптимизация гиперпараметров Pipeline V1 с Optuna

**Оптимизированные параметры:**
- `n_features` (SelectKBest): 90
- `n_estimators`: 200
- `max_depth`: 19
- `min_samples_split`: 4
- `min_samples_leaf`: 2
- `max_features`: 'sqrt'

**Результаты (после оптимизации):**
- **ROC-AUC:** 0.996359 (+0.015)
- **Accuracy:** 0.990778
- **F1-Score:** 0.990780

**Вывод:** Оптимизация гиперпараметров критически важна - улучшение на **+0.015** ROC-AUC!

#### 5.4 Pipeline V2: XGBoost + KNNImputer + StandardScaler + Target Encoding

**Архитектура:**
- Preprocessing:
  - Численные признаки: KNNImputer + StandardScaler
  - Категориальные признаки: Target Encoding
- Model: XGBoostClassifier

**Результаты:**
- **ROC-AUC:** 0.996311
- **Accuracy:** 0.988381
- **F1-Score:** 0.988384

#### 5.5 Создание ансамбля моделей (Blending)

**Стратегия:**
- Blending двух лучших моделей:
  - Pipeline V1 Optimized (RandomForest)
  - Pipeline V2 (XGBoost)
- Взвешенное усреднение вероятностей

**Результаты ансамбля:**
- **ROC-AUC:** 0.996463 (лучший результат!)
- **Accuracy:** 0.989756
- **F1-Score:** 0.989758

#### 5.6 Финальные результаты

| Модель | ROC-AUC | Accuracy | F1-Score | Kaggle Score |
|--------|---------|----------|----------|--------------|
| **LAMA Baseline** | 0.996311 | 0.988700 | 0.988702 | **0.98224** |
| **XGBoost Optimized (Ensemble)** | **0.996463** | **0.989756** | **0.989758** | **0.98456** |
| Pipeline v1 Optimized | 0.996359 | 0.990778 | 0.990780 | - |
| Pipeline v2 | 0.996311 | 0.988381 | 0.988384 | - |
| Pipeline v1 (до оптимизации) | 0.981060 | 0.930942 | 0.930945 | - |

**Выводы:**
1.  **Собственные модели показали отличные результаты**
2.  **XGBoost превзошел LAMA локально и на Kaggle** - Kaggle Score **0.98456 vs 0.98224** (+0.00232)
3.  **Ансамбль моделей улучшил результат** - blending дал ROC-AUC 0.996463
4.  **Оптимизация гиперпараметров критически важна** - улучшение с 0.981 до 0.996 (+0.015)
5.  **Различные подходы к preprocessing** - Target Encoding показал хорошие результаты
6.  **Feature Selection важен** - SelectKBest с оптимальным k значительно улучшил результаты

**Ключевые инсайты:**
- **RobustScaler vs StandardScaler:** RobustScaler показал лучшие результаты из-за наличия выбросов
- **One-Hot vs Target Encoding:** Target Encoding дал сопоставимые результаты при меньшей размерности
- **RandomForest vs XGBoost:** XGBoost показал лучшие результаты после оптимизации
- **Оптимальные гиперпараметры:** n_features=90, n_estimators=200, max_depth=19 для RandomForest

---

## Выводы по критериям

### 1. Анализ целевой переменной

 **Численный анализ:**
- Проведен полный анализ распределения классов
- Выявлено, что классы сбалансированы (разница 9.42%)
- Проверка на аномальные значения выполнена

 **Визуализация статистик:**
- Созданы pie chart и bar chart распределения классов
- Визуализации сохранены и документированы

### 2. Анализ признаков

 **Типизация признаков:**
- Идентифицированы 3 численных и 17 категориальных признаков
- Проанализированы распределения всех признаков

 **Выявление аномальных значений:**
- Использованы IQR и Z-score методы
- Созданы визуализации (box plots)

 **Анализ зависимостей между признаками:**
- Корреляция для численных признаков
- Cramér's V для категориальных
- ANOVA для смешанного анализа

 **Анализ пропущенных значений:**
- Проверены все признаки

 **Определение важности признаков:**
- Комбинированный подход: корреляция + Mutual Information
- Создан рейтинг важности признаков

 **Графическая визуализация:**
- Гистограммы, box plots, корреляционные матрицы
- Визуализации распределений категориальных признаков

 **Анализ преобразований и генерации признаков:**
- Проанализированы логарифмирование, стандартизация
- Предложены идеи для feature engineering

### 3. Моделирование

 **Обоснование стратегии разделения данных:**
- Stratified 80/20 split с обоснованием
- Проверка на data leakage

 **LAMA бейзлайн:**
- 2 различные конфигурации протестированы
- Выбрана лучшая конфигурация (Config 1)
- Kaggle Score: 0.98224

 **Собственное решение:**
- Построены 2 различных pipeline (V1 и V2)
- Оптимизация гиперпараметров с Optuna
- Создан ансамбль моделей (blending)
- **Результат превзошел LAMA baseline**

### 4. Общие требования к коду

 **Чистый код:**
- Ноутбук структурирован и оформлен
- Соответствие PEP 8
- Правильное именование переменных и функций
- Документирование функций (docstrings)

 **Качество кода:**
- Следование принципам SOLID
- Отсутствие спагетти-кода
- Обработка предупреждений и ошибок
- Логирование всех этапов

 **Структура решения:**
- Оформление в виде self-contained pipeline
- Использование sklearn Pipeline
- Модульная структура (pipeline_utils.py)

## Запуск проекта

1. Установите зависимости:
```bash
pip install -r requirements.txt
```

2. Убедитесь, что данные находятся в `playground-series-s4e8/`

3. Запустите ноутбук `mushrooms.ipynb` последовательно по ячейкам

4. Результаты будут сохранены в директории `results/`

